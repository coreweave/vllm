{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d33376-e1cf-41cd-a047-90e294ba0bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-10 20:30:49,498\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from tensorizer import TensorDeserializer, TensorSerializer, stream_io\n",
    "from tensorizer.utils import convert_bytes, get_mem_usage, no_init_or_tensor\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers.models.llama import LlamaConfig\n",
    "from vllm.model_executor.models.llama import LlamaForCausalLM\n",
    "from vllm.model_executor.models.mistral import MistralForCausalLM\n",
    "from vllm.model_executor.models.gpt_j import GPTJForCausalLM\n",
    "from vllm.model_executor.models.gpt_neox import GPTNeoXForCausalLM\n",
    "\n",
    "from vllm.model_executor.parallel_utils.parallel_state import \\\n",
    "    initialize_model_parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9b817-b71f-4a90-b8de-2583aa0dc3b2",
   "metadata": {},
   "source": [
    "Create a mapping between a model reference to the corresponding `vllm` module. This is *not* an exhaustive mapping, and just a demonstration of tensorizing a `vllm` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880aeb0b-2dc0-475f-8c7a-edba9b11ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral = [\"mistralai/Mistral-7B-v0.1\", \n",
    "           \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "           MistralForCausalLM]\n",
    "\n",
    "llama = [\"meta-llama/Llama-2-13b-hf\", \n",
    "\"meta-llama/Llama-2-70b-hf\", \n",
    "\"openlm-research/open_llama_13b\", \n",
    "\"lmsys/vicuna-13b-v1.3\", \n",
    "\"young-geng/koala\",\n",
    "LlamaForCausalLM]\n",
    "\n",
    "gptj = [\"EleutherAI/gpt-j-6b\", \"nomic-ai/gpt4all-j\", GPTJForCausalLM]\n",
    "\n",
    "gptneox = [\"EleutherAI/gpt-neox-20b\", \n",
    "\"EleutherAI/pythia-12b\", \n",
    "\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", \n",
    "\"databricks/dolly-v2-12b\", \n",
    "\"stabilityai/stablelm-tuned-alpha-7b\", \n",
    "GPTNeoXForCausalLM]\n",
    "\n",
    "modelref_to_module = {}\n",
    "for lists in [mistral, llama, gptj, gptneox]:\n",
    "    module = lists[-1]\n",
    "    for ref in lists[0:-1]:\n",
    "        modelref_to_module.update({ref:module})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d7cf01-dc89-44a4-bf83-7ad1c1db56fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate --quiet\n",
    "\n",
    "MODEL_REF = \"EleutherAI/gpt-j-6b\"\n",
    "BUCKET = \"tensorized-ssteel\"\n",
    "dtype = None\n",
    "\n",
    "if MODEL_REF not in modelref_to_module.keys():\n",
    "    raise KeyError(f\"{MODEL_REF} not in supported model list given.\")\n",
    "\n",
    "MODEL_NAME = MODEL_REF.split(\"/\")[1]\n",
    "S3_URI = f\"s3://{BUCKET}/{MODEL_NAME}-vllm.tensors\"\n",
    "\n",
    "MODEL_PATH = f\"/tmp/{MODEL_NAME}\"\n",
    "\n",
    "if dtype:\n",
    "    torch.set_default_dtype(dtype)\n",
    "\n",
    "def make_model_contiguous(model):\n",
    "    # Ensure tensors are saved in memory contiguously\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "def serialize():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_REF, device_map=\"auto\", torch_dtype=\"auto\"\n",
    "    )\n",
    "    \n",
    "    make_model_contiguous(model)\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    config = AutoConfig.from_pretrained(MODEL_REF)\n",
    "    model = modelref_to_module[MODEL_REF](config)\n",
    "    model.load_weights(MODEL_PATH)\n",
    "\n",
    "    stream = stream_io.open_stream(S3_URI, \"wb\")\n",
    "    serializer = TensorSerializer(stream)\n",
    "    print(f\"Writing serialized tensors for model {MODEL_REF} using module {model} to {S3_URI}.\") \n",
    "    serializer.write_module(model)\n",
    "    serializer.close()\n",
    "    print(\"Serialization complete. It is recommended you restart the kernel if deserializing.\")\n",
    "\n",
    "\n",
    "def deserialize():\n",
    "    config = AutoConfig.from_pretrained(MODEL_REF)\n",
    "\n",
    "    with no_init_or_tensor():\n",
    "        model = modelref_to_module[MODEL_REF](config)\n",
    "        if dtype: \n",
    "            model.to(dtype)\n",
    "\n",
    "    before_mem = get_mem_usage()\n",
    "    # Lazy load the tensors from S3 into the model.\n",
    "    start = time.time()\n",
    "    stream = stream_io.open_stream(S3_URI, \"rb\")\n",
    "    deserializer = TensorDeserializer(stream, plaid_mode=True)\n",
    "    deserializer.load_into_module(model)\n",
    "    end = time.time()\n",
    "\n",
    "    # Brag about how fast we are.\n",
    "    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n",
    "    duration = end - start\n",
    "    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n",
    "    after_mem = get_mem_usage()\n",
    "    deserializer.close()\n",
    "    print(f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\")\n",
    "    print(f\"Memory usage before: {before_mem}\")\n",
    "    print(f\"Memory usage after: {after_mem}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb38aef9-5a72-43b7-9f84-8cf0ae976263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-j-6b\n",
      "/tmp/gpt-j-6b\n",
      "s3://tensorized-ssteel/gpt-j-6b-vllm.tensors\n"
     ]
    }
   ],
   "source": [
    "print(f\"{MODEL_NAME}\\n{MODEL_PATH}\\n{S3_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0e2ea0-b142-47d1-85df-f69989f07ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MASTER_ADDR\"] = \"0.0.0.0\"\n",
    "os.environ[\"MASTER_PORT\"] = \"8080\"\n",
    "\n",
    "torch.distributed.init_process_group(world_size=1, rank=0)\n",
    "initialize_model_parallel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b63268-f913-454b-a42b-5b08eb30ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing...\n",
      "Writing serialized tensors for model EleutherAI/gpt-j-6b using module GPTJForCausalLM(\n",
      "  (transformer): GPTJModel(\n",
      "    (wte): VocabParallelEmbedding()\n",
      "    (h): ModuleList(\n",
      "      (0-27): 28 x GPTJBlock(\n",
      "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTJAttention(\n",
      "          (qkv_proj): QKVParallelLinear()\n",
      "          (out_proj): RowParallelLinear()\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (attn): PagedAttention()\n",
      "        )\n",
      "        (mlp): GPTJMLP(\n",
      "          (fc_in): ColumnParallelLinear()\n",
      "          (fc_out): RowParallelLinear()\n",
      "          (act): NewGELU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): ParallelLMHead()\n",
      "  (sampler): Sampler()\n",
      ") to s3://tensorized-ssteel/gpt-j-6b-vllm.tensors.\n",
      "Serialization complete. It is recommended you restart the kernel if deserializing.\n"
     ]
    }
   ],
   "source": [
    "print(\"Serializing...\")\n",
    "serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4e5ad1-cb3a-4120-a343-00111e1f19bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deserializing...\n",
      "Deserialized 24.2 GB in 292.81s, 82.7 MB/s\n",
      "Memory usage before: CPU: (maxrss: 4,081MiB F: 95,826MiB) GPU: (U: 867MiB F: 47,809MiB T: 48,676MiB) TORCH: (R: 2MiB/2MiB, A: 0MiB/1MiB)\n",
      "Memory usage after: CPU: (maxrss: 6,114MiB F: 95,813MiB) GPU: (U: 23,971MiB F: 24,705MiB T: 48,676MiB) TORCH: (R: 23,086MiB/23,086MiB, A: 23,083MiB/23,084MiB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Deserializing...\")\n",
    "model = deserialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f78d75-fcfb-4a9f-b3e5-afac5a65bd0f",
   "metadata": {},
   "source": [
    "Tensorizing a model in bfloat16 forces you to specify `dtype=\"bfloat16\"` in the api_server.py CLI, as it gets defaults from Llama 2's HF config, which goes for float16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
