{{- range $v := .Values.deployments }}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ $v.load_format }}{{ $v.plaid_mode }}{{ $v.lazy_load}}
spec:
  template:
    spec:
      containers:
        - name: run-benchmark
          image: nvcr.io/nvidia/pytorch:23.10-py3
          command:
            - "/bin/sh"
            - "-c"
            - |
              apt-get update && apt-get install -y python3-pip git s3cmd
              cd / && git clone --branch sangstar/benchmarking https://github.com/coreweave/vllm.git 
              cd vllm
              export MAX_JOBS=8
              pip install .
              num_iterations=50
              
              unique_id=$(date +%s)  # generate a unique identifier based on the current timestamp
              models="EleutherAI/pythia-1.4b mistralai/Mistral-7B-v0.1 meta-llama/Llama-2-13b-hf"
              download_dir=~/.cache/weights
              python_command="python vllm/entrypoints/openai/api_server.py --host 0.0.0.0 --load-format {{ $v.load_format }}"
              if [ "{{ $v.load_format }}" = "tensorizer" ]; then
                python_command="$python_command {{ $v.lazy_load }}"
                if [ "$model" = "meta-llama/Llama-2-13b-hf" ]; then
                  if [ -n "{{ $v.plaid_mode }}" ]; then
                    tensorizer_uri="s3://tensorized-ssteel/Llama-2-13b-hf-vllm.tensors"
                    python_command="$python_command {{ $v.plaid_mode }}"
                  else
                    tensorizer_uri="s3://tensorized-ssteel/meta-llama/Llama-2-13b-hf/model.tensors"
                  fi
                fi
                python_command="$python_command --tensorizer-uri $tensorizer_uri"
              else
                python_command="$python_command --download-dir $download_dir"
              fi
              tensorizer_downloaded=false
              for model in $models; do
                for i in $(seq 1 $num_iterations); do
                  echo "========================================================"
                  echo "Running iteration ${i} for model $model"
                  echo "========================================================"
                  if [ $i -eq $((num_iterations/2 + 1)) ] && [ "{{ $v.load_format }}" = "tensorizer" ] && [ "$tensorizer_downloaded" = false ]; then
                    # Download the tensorizer_uri locally with s3
                    s3cmd get $tensorizer_uri $download_dir/
                    # Specify the local path as the new tensorizer_uri
                    tensorizer_uri="$download_dir/$(basename $tensorizer_uri)"
                    python_command="${python_command/--tensorizer-uri s3:\/\/tensorized-ssteel\/Llama-2-13b-hf-vllm.tensors/--tensorizer-uri $tensorizer_uri}"
                    tensorizer_downloaded=true
                  fi
                  eval $python_command --model $model
                  if [ $i -le $((num_iterations/2)) ] && [ "{{ $v.load_format }}" != "tensorizer" ]; then
                    rm -rf $download_dir
                  fi
                  if [ $(($i % 10)) -eq 0 ]; then
                    s3_path="s3://tensorized-ssteel/vllm-benchmarks/$unique_id/{{ $v.load_format }}/$model"
                    if [ $i -lt $((num_iterations/2 + 1)) ]; then
                      s3_path="$s3_path/non_local"
                    else
                      s3_path="$s3_path/local"
                    fi
                    if [ -n "{{ $v.lazy_load }}" ]; then
                      s3_path="$s3_path/{{ $v.lazy_load }}"
                    fi
                    if [ -n "{{ $v.plaid_mode }}" ]; then
                      s3_path="$s3_path/{{ $v.plaid_mode }}"
                    fi
                    s3cmd put results_lazy_load.csv $s3_path/results_checkpoint_${i}_vals.csv --config /root/s3cfg/.s3cfg
                  fi
                done
              done

          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-secret
                  key: token
            - name: DEBIAN_FRONTEND
              value: noninteractive
          volumeMounts:
            - name: s3cfg
              mountPath: /root/s3cfg
          resources:
            requests:
              cpu: "32"
              memory: 128Gi
              nvidia.com/gpu: "1"
            limits:
              cpu: "32"
              memory: 128Gi
              nvidia.com/gpu: "1"
      volumes:
        - name: s3cfg
          secret:
            secretName: s3cfg-raw
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                  - "ORD1"
              - key: gpu.nvidia.com/class
                operator: In
                values:
                  - "A40"
      restartPolicy: OnFailure
---
{{- end }}
